---
title: "AI Refactor Project 2 "
author: "Destinee Ramos"
format:
  html:
    fig-number: true
execute:
  echo: true
  warning: true
  message: true
---

Summary of re factoring choices

- Most of my re factoring included getting rid of for loops and replacing them with vectorized code. Getting rid of a for loop in both summarize_behavior and build_participant_wide allowed me to better optimize my code and make it as short and practical as possible. In R, code will automatically loop through rows, therefore, so that others can best read and reproduce my code, I decided to get rid of for loops. Vectorized versions are much shorter and simpler (in my opinion). My third place where I re factored code was in summarize_behavior when I got rid of matrices in the code, and instead, used subset to index by name (instead of position). I did this because when working with large data sets and passing them between peers and colleague, indexing by position can be dangerous, as if one thing gets put out of place, it is easy to move forward analyzing the completely wrong variable Therefore, indexing my name instead makes the code easier to read (you know exactly what variable you're referring to) and safer to use as large data sets are modified and passed along.  

Re factoring #1 in build_participant_wide (for loop into vectorized version)
```{r}
##OLD CODE 
if (FALSE) {
  rows <- list()
  for (i in seq_along(files)) {
   file_name <- files[i]
   rows[[i]] <- import_and_process(file_name)
  }
}

##NEW CODE 
if (FALSE) {
    rows <- lapply(files, import_and_process)
}

```
 
Re factoring #2 in summarize_behavior (for loop into vectorized version)
```{r}

##OLD CODE 
if (FALSE) {
  valid_data_rt$rt_centered <- NA_real_
  for (i in 1:nrow(valid_data_rt)) {
  valid_data_rt$rt_centered[i] <- valid_data_rt$rt[i] - mean(valid_data_rt$rt, na.rm = TRUE)
  }
}

##NEW CODE
if (FALSE) {
  valid_data_rt$rt_centered <- valid_data_rt$rt - mean(valid_data_rt$rt, na.rm = TRUE)
}

```
 
 
 Re factoring #3 in summarize_behvaior (index by name, instead of position) 
```{r}
## OLD CODE 
if (FALSE) {
  valid_data_rt <- data[data[ , 3] >= 300 & data[ , 3] <= 900 & data[ , 12] == TRUE, ]
  valid_data_acc <- data[data[ , 3] >= 300 & data[ , 3] <= 900, ]
}

##NEW CODE 
if (FALSE) {
  valid_data_rt <- subset(data, rt >= 300 & rt <= 900 & correct == TRUE)
  valid_data_acc <- subset(data, rt >= 300 & rt <= 900)
}

```


AI Optimization   


Ideas AI gave for unedited project 

- Always use here:here and standardize file paths. Points out situations where I use relative paths, instead of absolute paths. This breaks the code if someone else trys to reproduce it. By using only absolute paths (here:here), it helps reduce path-related errors (which Chat says is "the #1 beginner issue")

- Instead of repeated subsetting, use mini functions to acheive the same goals.This helps remove repetitve code across files and makes the code more readable for beginners. Chat recommends instead of doing code like this 
```{r}
if (FALSE) {
  valid <- data[data$rt >= rt_min & data$rt <= rt_max, ]
}
```

to do this instead 

```{r}
if (FALSE) {
  subset_valid_rt <- function(df, rt_min, rt_max) {
  df[df$rt >= rt_min & df$rt <= rt_max, , drop = FALSE]
}
}
```

- Use lapply instead of manual for loops. Chat recommends to use the apply family (e.g. lapply) instead of for loops to make the code shorter, less error-prone, and more consistent. 

It recommends that instead of this 
```{r}
if (FALSE) {
  for (file in files) {
  df <- read.csv(file)
  summary <- summarize_behavior(df)
  result_list[[file]] <- summary
}
}
```

Use this: 
```{r}
if (FALSE) {
  summaries <- lapply(files, function(path) {
  df <- read.csv(path)
  validate_iat_df(df)
  summarize_behavior(df)
})
summaries_df <- do.call(rbind, summaries)
}
```

Ideas AI gave for re factored version 

- Move all of the different warnings and checks into one function. This helps a beginner looking at your code to see exactly what checks are done throughout without gong and searching for them. It also ensure no duplication and allows you to easily add checks in one place. 

Instead of this: 

```{r}
##I currently duplicate many checks:
if (FALSE) {
if (!is.numeric(data$rt)) { ... }
if (!all(required_cols %in% names(df))) { ... }
if (!is.logical(data$correct)) { ... }
}
```

Chat recommends that hte function be created like this: 
```{r}
##In a file called validate_task_df.R:

if (FALSE) {
validate_task_df <- function(df) {
  required <- c("trialType","block","rt","response",
                "expectedCategory","expectedCategoryAsDisplayed",
                "correct")
  missing <- required[!required %in% names(df)]
  if (length(missing) > 0) stop("Missing columns: ", paste(missing, collapse=", "))

  if (!is.numeric(df$rt)) stop("'rt' must be numeric.")
  if (!all(df$correct %in% c(0,1,TRUE,FALSE), na.rm = TRUE))
    stop("'correct' must be coded 0/1 or TRUE/FALSE")
  
  invisible(TRUE)
}
##Then at the top of each script:
  validate_task_df(data)
}
```

- Repalce repeated subset(). Instead use a helper function for duplicated RT filtering. Instead of using subset multiple times and then ahvig to repeat your bounds when you call the function, it is helpful to create a new functino inside of summarize_behavior that dictates the bounds. For exmaple, instead of this 

```{r}
if (FALSE) {

valid_data_rt  <- subset(data, rt >= 300 & rt <= 900 & correct == TRUE)
valid_data_acc <- subset(data, rt >= 300 & rt <= 900)

summarize_behavior <- function(data, rt_min = 300, rt_max = 900) {}
}
```

Use this: 
```{r}
if (FALSE) {
  within_rt_bounds <- function(df) {
  df[df$rt >= rt_min & df$rt <= rt_max, , drop = FALSE]
}

valid_data <- within_rt_bounds(data)
valid_data_rt  <- valid_data[valid_data$correct == TRUE, ]
valid_data_acc <- valid_data  # accuracy doesnâ€™t filter correct=TRUE

}

```

- Wrap JSON parsing in lapply. Right now, my code to parse the JSON string will only parse the first line, silently ignoring the rest. However, if we were working with data that had multiple lines needing to be parsed,we would want to use lapply inside of score_questionnaire.

Instead of this: 
```{r}

if (FALSE) {
responses <- jsonlite::fromJSON(as.character(json_string))

##But in import_and_process.R:
anxiety_score <- score_questionnaire(questionnaire_df$response)
}
```

Replace it with this: 
```{r}

if (FALSE) {
  ##Inside score_questionnaire():
if (length(json_string) > 1) {
  responses <- unlist(lapply(json_string, function(x) {
    out <- jsonlite::fromJSON(as.character(x))
    unlist(out)
  }))
} else {
  responses <- jsonlite::fromJSON(as.character(json_string))
  responses <- unlist(responses)
}
}

```



Comparison section: 

Some of my own changes matched AI's ideas, but most didn't. In my re factoring, I got rid of manual for loops, and AI did in theirs as well. However, many of the changes AI made did not match. For example, they included that I should only use absolute paths (here:here), instead of relative, which maeks a lot of sense to me when thinking about how others can duplicate my proejcts. AI also gave me a lot of suggestions to remove repeated code by creating mini helper functions that do one little job, but can be used a lot. I had really only thought of having a fwe functions per project, but this AI overview is showing me a lot of different ways you can use functions at all times. I especially liked the idea of putting all of the checks into one function, as I could see how that could get confusing with a large project. I did find it hard to understand a lot fo AI's suggestions, especially in regards to making all of these functions. It was concepts we had learned, but things that I would've never though to put together and that I couldn't fully explain what they did. Overall, I learned that there are a lot of ways to shorten up code and improve workflow, and a lot of this comes with making super modular, seperate code where each block has a specific job. 



